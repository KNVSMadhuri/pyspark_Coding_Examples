#Reading multiple Parquet files from a folder and creating Dataframe.
df = sqlContext.read.parquet('/home/affine/Desktop/Train1/*.parquet')

# 1.How many hotel cluster are available in data
df1 = df.select('hotel_cluster').distinct().collect()
df1

#To Print Schema.
df.printSchema()

#2. How many search destination types are available in  srch_destination_type_id column
df.groupby('srch_destination_type_id').count().collect()

# 3. Percentage conversion for each hotel cluster using pyspark SQL
df.registerTempTable("df")
Percentage_conversion = sqlContext.sql("""
  SELECT hotel_cluster, COUNT(*) total, SUM(is_booking) total_is_booking, 
         CAST(AVG(is_booking) * 100 AS INT) percentage
  FROM df
  GROUP BY hotel_cluster""")
Percentage_conversion.show()

#4. Percentage conversion for each hotel cluster using pyspark Dataframe.
from pyspark.sql.functions import mean,count,sum
df.groupby(['hotel_cluster']).agg(count('*').alias('no. of search queries converted')
                                  ,sum('is_booking').alias('Total search queries')
                                  ,(mean('is_booking') * 100).cast('integer').alias('percentage')) .show()
                                  
#Working with dictionary elements  
d = {}
a=int(input()) 
for i in range(a): 
    b = input() 
    c = b.split(" ") 
    su = float(c[1]) + float(c[2]) + float(c[3]) 
    av = float(su/3) 
    d[c[0]] = av
    e = input() 
print ('%.2f' % d[e])
                                  
#5. Calculating top 5 elements based on is_booking.
from pyspark.sql.functions import sum
distribution_aggregation = df.groupby(['hotel_cluster']).agg(sum('is_booking').alias('is_booking'),
                                  sum('srch_adults_cnt').alias('srch_adults_cnt'),
                                 sum('srch_children_cnt').alias('srch_children_cnt'),
                                 sum('srch_rm_cnt').alias('srch_rm_cnt'),
                                 sum('orig_destination_distance').alias('orig_destination_distance'),
                                 sum('site_name').alias('site_name'),
                                 sum('hotel_market').alias('hotel_market'),
                                 sum('srch_destination_type_id').alias('srch_destination_type_id'),
                                 sum('hotel_country').alias('hotel_country'))

                                 
distribution_aggregation.sort('is_booking', ascending=False).show(5)                                 
                                  
#5. Plotting graphs using pyspark

%pylab inline
subPercent = Percentage.toPandas() 
subPercent[:20].plot(x='hotel_cluster', y='percentage', kind='barh', alpha=0.5)  

#Find the top 5 hotel cluster having maximum booking
Percentage.sort('no_of_search_queries_converted', ascending=False).show(5)

#5. Calculating top 5 elements based on is_booking.
from pyspark.sql.functions import sum
df.filter('hotel_cluster = 91').groupby(['srch_adults_cnt']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['srch_children_cnt']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['srch_rm_cnt']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['orig_destination_distance']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['site_name']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['hotel_market']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['srch_destination_type_id']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['hotel_continent']).agg(sum('is_booking')).show()
df.filter('hotel_cluster = 91').groupby(['hotel_country']).agg(sum('is_booking')).show()


#dealing with dictionaries
d = {}
a=int(input()) 
for i in range(a): 
    b = input() 
    c = b.split(" ") 
    su = float(c[1]) + float(c[2]) + float(c[3]) 
    av = float(su/3) 
    d[c[0]] = av
    e = input() 
print ('%.2f' % d[e])


#Printing Style
from  __future__  import print_function
import sys
n = int(input())
for i in range(1,n+1):
    print(i,sep=' ', end='', file=sys.stdout)
##dealing with commands append, remove,pop,runtime
n = input()
l = []
for _ in range(n):
    s = input().split()
    cmd = s[0]
    args = s[1:]
    if cmd !="print":
        cmd += "("+ ",".join(args) +")"
        eval("l."+cmd)
    else:
        print(l)
#Dynamically assigning variables:
raw_input()
print(hash(tuple(map(int, raw_input().strip().split(" ")))))

#using lexicographic variables:
x, y, z, n = (int(input()) for _ in range(4))
print ([[a,b,c] for a in range(x+1) for b in range(y+1) for c in range(z+1) if a + b + c != n ])

#Finding second largest number:

raw_input()
l = set(list(map(int,raw_input().strip().split(" "))))
m = sorted(l,reverse=True)[1]
print(m)

#Finding number of lowest scorers with same name

n = int(raw_input())
marksheet = [[raw_input(), float(raw_input())] for _ in range(n)]
second_highest = sorted([marks for name,marks in marksheet])[1]
print('\n'.join([a for a,b in sorted(marksheet) if b == second_highest]))

OR

students = sorted([[raw_input(), float(input())] for _ in range(int(input()))])
secndLow = sorted(list(set(grade for [_, grade] in students)))[1]
print ("\n".join( name for [name, grade] in students if grade == secndLow))

#t1-t2 dynamically timedelta problem
import re
def parseString(s):
    (x1,x2) = re.findall('(^.*) ([+-]\d{4})',s)[0]
    tz = x2[0] + str(int(x2[1:3])*3600 + int(x2[3:5])*60)
    ts = time.strptime(x1,"%a %d %b %Y %H:%M:%S")
    return time.mktime(ts) - eval(tz)
for i in range(int(input())):
    print(int(abs(parseString(input()) - parseString(input()))))
